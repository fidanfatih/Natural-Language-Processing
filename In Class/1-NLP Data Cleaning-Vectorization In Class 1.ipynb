{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Install and Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sample_text= \"Oh man, this is pretty cool. We will do more such things. Don't enjoy. 2\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
    "# sent_tokenize sentence tokenize demek. Genellikle word_tokenize kullanılıyor, yani bütün text kelime kelime ayrılıyor.  \r\n",
    "# Not: Eğer döküman cümle cümle ayrılmışsa her bir cümle token olarak adlandırılır, kelime kelime ayrılmışsa her bir kelime token olarak adlandırılır. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#nltk.download('punkt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sentence_token = sent_tokenize(sample_text.lower())\r\n",
    "sentence_token\r\n",
    "# Tüm datamızdaki aynı olan verileri karşılaştırabilmek için bütün kelimeleri küçük harflere dönüştürüyoruz.\r\n",
    "# Cümle olarak ayrılan token ler"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh man, this is pretty cool.',\n",
       " 'we will do more such things.',\n",
       " \"don't enjoy.\",\n",
       " '2']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "word_token = word_tokenize(sample_text.lower())\r\n",
    "word_token\r\n",
    "# Kelime olarak ayrılan token ler"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " '.',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'enjoy',\n",
       " '.',\n",
       " '2']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing Punctuation and Numbers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()]\r\n",
    "tokens_without_punc\r\n",
    "# Noktalama ve sayıları datamızdan çıkarıyoruz.\r\n",
    "# w.isalpha() datadaki string değerleri seçmeye yarıyor, sayı ve noktalama işaretlerini token edilmiş datadan çıkarıyor. Eğer sayılarda bizim için önemli ise .isalnum() kullanmalıyız. .isalnum() string ve numeric değerleri alıyor. "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'do',\n",
       " 'enjoy']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing Stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#nltk.download('stopwords')\r\n",
    "# stopwords bir dilde en fazla kullanılan soru, bağlaç, özne vb. kelimeler bu kelimeler cümle içerisinde çok fazla bulunduğu için modelin kelimeleri değerlendirirken modelde yanlış değerlendirmelere neden olabileceğinden genellikle düşürülmesi tercih edilir.   "
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "stopwords.words(\"english\")\r\n",
    "# words(\"english\") datamız ingilizce olduğu için ingilizcede ki stopwords leri çağırıyoruz."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tokens_without_punc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'do',\n",
       " 'enjoy']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "token_without_sw = [t for t in tokens_without_punc if t not in stop_words]\r\n",
    "token_without_sw\r\n",
    "# stopwords lari datamızdan çıkardık."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'things', 'enjoy']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "[i for i in stop_words if \"n't\" in i]\r\n",
    "# Eğer olumsuz kelimeler bizim için önemli ise olumsuz stopwords ları bu kod ile ayrıştırabiliriz. Burada stopwords datamızdaki Don't u düşürdüğü için kelimenin olumsuz anlamı yok oldu.\r\n",
    "# Not : Bert ve benzeri deep learning ile geliştirilmiş modelleri kullanıyorsanız stopword leri temizlememize gerek yoktur. Çünkü bu modeller bunlar için gerekli işlemleri otomatik yapıyorlar."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"don't\",\n",
       " \"aren't\",\n",
       " \"couldn't\",\n",
       " \"didn't\",\n",
       " \"doesn't\",\n",
       " \"hadn't\",\n",
       " \"hasn't\",\n",
       " \"haven't\",\n",
       " \"isn't\",\n",
       " \"mightn't\",\n",
       " \"mustn't\",\n",
       " \"needn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"won't\",\n",
       " \"wouldn't\"]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#nltk.download('wordnet')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "# Lemmatization kelimenin kökenine inmeden değerlendirmeye alıyor. Bu yüzden genellikle Lemmatization kullanılıyor. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "WordNetLemmatizer().lemmatize(\"driving\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'driving'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t) for t in token_without_sw]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "lem"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'thing', 'enjoy']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from nltk.stem import PorterStemmer\r\n",
    "# Stemming kelimenin kökenine indiği için her zaman kelimenin tam anlamını veremeyebilir."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "PorterStemmer().stem(\"driving\")\r\n",
    "# Bizim kelimemiz isim anlamında olduğu halde stemmer onun köküne inerek fiil anlamını getirdirde ve anlam değişikliğine neden oldu. "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "stem = [PorterStemmer().stem(t) for t in token_without_sw]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "stem"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretti', 'cool', 'thing', 'enjoy']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\" \".join(lem)\r\n",
    "# Lemmatizer halindeki kelimelerimizi aralarında bir boşluk(\" \" sağlıyor) olacak şekilde birleştirdik."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'oh man pretty cool thing enjoy'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\" \".join(stem)\r\n",
    "# Stemmer halindeki kelimelerimizi aralarında bir boşluk(\" \" sağlıyor) olacak şekilde birleştirdik."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'oh man pretti cool thing enjoy'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\", \".join(stem)\r\n",
    "# Kelimeler arasına virgül ve boşluk ekledik."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'oh, man, pretti, cool, thing, enjoy'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "#from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
    "#from nltk.corpus import stopwords\r\n",
    "#stop_words = stopwords.words(\"english\")\r\n",
    "#from nltk.stem import WordNetLemmatizer\r\n",
    "def cleaning(data):\r\n",
    "    #1. Tokenize\r\n",
    "    text_tokens = word_tokenize(data.lower())\r\n",
    "    #2. Remove Puncs\r\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\r\n",
    "    #3. Removing Stopwords\r\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\r\n",
    "    #4. lemma\r\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\r\n",
    "    #joining\r\n",
    "    return \" \".join(text_cleaned)\r\n",
    "# Yukarıda yaptığımız işlemleri tek bir fonksiyona dönüştürüyoruz."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "pd.Series(sample_text).apply(cleaning)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    oh man pretty cool thing enjoy\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count Vectorization and TF-IDF Vectorization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df = pd.read_csv(\"airline_tweets.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "df = df[['airline_sentiment','text']]\r\n",
    "df\r\n",
    "# Datasetin den havayolu ile ilgili yorum ve değerlendirmeleri seçiyoruz."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "0               neutral                @VirginAmerica What @dhepburn said.\n",
       "1              positive  @VirginAmerica plus you've added commercials t...\n",
       "2               neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3              negative  @VirginAmerica it's really aggressive to blast...\n",
       "4              negative  @VirginAmerica and it's a really big bad thing...\n",
       "...                 ...                                                ...\n",
       "14635          positive  @AmericanAir thank you we got on a different f...\n",
       "14636          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14637           neutral  @AmericanAir Please bring American Airlines to...\n",
       "14638          negative  @AmericanAir you have my money, you change my ...\n",
       "14639           neutral  @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "\n",
       "[14640 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "df = df.iloc[:8, :]\r\n",
    "df\r\n",
    "# İlk sekiz satırı alıyoruz."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "7           neutral  @VirginAmerica Really missed a prime opportuni..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df2 = df.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "df2[\"text\"] = df2[\"text\"].apply(cleaning)\r\n",
    "# Yukarıdaki fonksiyonumuzu buradaki text sütununa uyguluyoruz. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "df2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                        virginamerica dhepburn said\n",
       "1          positive  virginamerica plus added commercial experience...\n",
       "2           neutral  virginamerica today must mean need take anothe...\n",
       "3          negative  virginamerica really aggressive blast obnoxiou...\n",
       "4          negative                 virginamerica really big bad thing\n",
       "5          negative  virginamerica seriously would pay flight seat ...\n",
       "6          positive  virginamerica yes nearly every time fly vx ear...\n",
       "7           neutral  virginamerica really missed prime opportunity ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus added commercial experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica today must mean need take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica seriously would pay flight seat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica really missed prime opportunity ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CountVectorization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "X = df2[\"text\"]\r\n",
    "y = df2[\"airline_sentiment\"]\r\n",
    "# Not: Vectorization kelimelerin anlamlarına göre değil kelimelerin cümle içerisinde birbirleri ile birlikte kullanılma olasılıklarına göre değerlendirme yapıyor. Bu yüzden genellikle Deep Learning modellerinde Lemmatization ve Stemming uygulanmıyor."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "vectorizer = CountVectorizer()\r\n",
    "X_train_count = vectorizer.fit_transform(X_train)\r\n",
    "X_test_count = vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "vectorizer.get_feature_names()\r\n",
    "# Corpus (tüm dataseti) taki bütün unique kelimeleri getiriyor."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['aggressive',\n",
       " 'amp',\n",
       " 'another',\n",
       " 'away',\n",
       " 'bad',\n",
       " 'big',\n",
       " 'blast',\n",
       " 'ear',\n",
       " 'entertainment',\n",
       " 'every',\n",
       " 'face',\n",
       " 'fly',\n",
       " 'go',\n",
       " 'guest',\n",
       " 'little',\n",
       " 'mean',\n",
       " 'must',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'obnoxious',\n",
       " 'really',\n",
       " 'recourse',\n",
       " 'take',\n",
       " 'thing',\n",
       " 'time',\n",
       " 'today',\n",
       " 'trip',\n",
       " 'virginamerica',\n",
       " 'vx',\n",
       " 'worm',\n",
       " 'yes']"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "X_test_count.dtype"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "X_train_count.toarray()\r\n",
    "# Herbir satırı (yorumu) sayısal değerlere dönüştürdü."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())\r\n",
    "# Her bir satırı unique kelimelere göre değerlendiriyoruz. Eğer unique kelimelerden biri o satırda bulunuyorsa 1 yoksa 0 olarak gözüküyor. "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   aggressive  amp  another  away  bad  big  blast  ear  entertainment  every  \\\n",
       "0           0    0        1     0    0    0      0    0              0      0   \n",
       "1           0    0        0     0    1    1      0    0              0      0   \n",
       "2           1    1        0     0    0    0      1    0              1      0   \n",
       "3           0    0        0     1    0    0      0    1              0      1   \n",
       "\n",
       "   face  fly  go  guest  little  mean  must  nearly  need  obnoxious  really  \\\n",
       "0     0    0   0      0       0     1     1       0     1          0       0   \n",
       "1     0    0   0      0       0     0     0       0     0          0       1   \n",
       "2     1    0   0      1       1     0     0       0     0          1       1   \n",
       "3     0    1   1      0       0     0     0       1     0          0       0   \n",
       "\n",
       "   recourse  take  thing  time  today  trip  virginamerica  vx  worm  yes  \n",
       "0         0     1      0     0      1     1              1   0     0    0  \n",
       "1         0     0      1     0      0     0              1   0     0    0  \n",
       "2         1     0      0     0      0     0              1   0     0    0  \n",
       "3         0     0      0     1      0     0              1   1     1    1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggressive</th>\n",
       "      <th>amp</th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>blast</th>\n",
       "      <th>ear</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>every</th>\n",
       "      <th>face</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>guest</th>\n",
       "      <th>little</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>obnoxious</th>\n",
       "      <th>really</th>\n",
       "      <th>recourse</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "X_train[6]\r\n",
    "# İlk kelimemiz (0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx ear worm go away'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "sklearn TD-IDF\n",
    "https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "# TF kelimelerin döküman içerindaki sayısına göre oran belirliyor. IDF ise kelimenin corpus taki sayısını ve toplam kelime sayısını kullanarak log alıyor. Böylece bir kelime kullanma sıklığı artıkça oranını düşürüyor ki kelime ağırlıkları dengelensin. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\r\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\r\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)\r\n",
    "# Burada fit corpus taki tüm kelimeleri alıp sayısını belirliyor, transform da ise TF ve IDF formülleri bütün kelimelere uygulanıyor. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "tf_idf_vectorizer.get_feature_names()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['aggressive',\n",
       " 'amp',\n",
       " 'another',\n",
       " 'away',\n",
       " 'bad',\n",
       " 'big',\n",
       " 'blast',\n",
       " 'ear',\n",
       " 'entertainment',\n",
       " 'every',\n",
       " 'face',\n",
       " 'fly',\n",
       " 'go',\n",
       " 'guest',\n",
       " 'little',\n",
       " 'mean',\n",
       " 'must',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'obnoxious',\n",
       " 'really',\n",
       " 'recourse',\n",
       " 'take',\n",
       " 'thing',\n",
       " 'time',\n",
       " 'today',\n",
       " 'trip',\n",
       " 'virginamerica',\n",
       " 'vx',\n",
       " 'worm',\n",
       " 'yes']"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "X_train_tf_idf.toarray()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.37082034, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.37082034, 0.37082034, 0.        , 0.37082034, 0.        ,\n",
       "        0.        , 0.        , 0.37082034, 0.        , 0.        ,\n",
       "        0.37082034, 0.37082034, 0.19350944, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.50676543,\n",
       "        0.50676543, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39953968, 0.        , 0.        , 0.50676543, 0.        ,\n",
       "        0.        , 0.        , 0.26445122, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.31791864, 0.31791864, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31791864, 0.        , 0.31791864, 0.        ,\n",
       "        0.31791864, 0.        , 0.        , 0.31791864, 0.31791864,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.31791864,\n",
       "        0.25065071, 0.31791864, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.16590314, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.31200802, 0.        ,\n",
       "        0.        , 0.        , 0.31200802, 0.        , 0.31200802,\n",
       "        0.        , 0.31200802, 0.31200802, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31200802, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.31200802,\n",
       "        0.        , 0.        , 0.16281873, 0.31200802, 0.31200802,\n",
       "        0.31200802]])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   aggressive       amp  another      away       bad       big     blast  \\\n",
       "0    0.000000  0.000000  0.37082  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.00000  0.000000  0.506765  0.506765  0.000000   \n",
       "2    0.317919  0.317919  0.00000  0.000000  0.000000  0.000000  0.317919   \n",
       "3    0.000000  0.000000  0.00000  0.312008  0.000000  0.000000  0.000000   \n",
       "\n",
       "        ear  entertainment     every      face       fly        go     guest  \\\n",
       "0  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000       0.317919  0.000000  0.317919  0.000000  0.000000  0.317919   \n",
       "3  0.312008       0.000000  0.312008  0.000000  0.312008  0.312008  0.000000   \n",
       "\n",
       "     little     mean     must    nearly     need  obnoxious    really  \\\n",
       "0  0.000000  0.37082  0.37082  0.000000  0.37082   0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.00000  0.000000  0.00000   0.000000  0.399540   \n",
       "2  0.317919  0.00000  0.00000  0.000000  0.00000   0.317919  0.250651   \n",
       "3  0.000000  0.00000  0.00000  0.312008  0.00000   0.000000  0.000000   \n",
       "\n",
       "   recourse     take     thing      time    today     trip  virginamerica  \\\n",
       "0  0.000000  0.37082  0.000000  0.000000  0.37082  0.37082       0.193509   \n",
       "1  0.000000  0.00000  0.506765  0.000000  0.00000  0.00000       0.264451   \n",
       "2  0.317919  0.00000  0.000000  0.000000  0.00000  0.00000       0.165903   \n",
       "3  0.000000  0.00000  0.000000  0.312008  0.00000  0.00000       0.162819   \n",
       "\n",
       "         vx      worm       yes  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  \n",
       "3  0.312008  0.312008  0.312008  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggressive</th>\n",
       "      <th>amp</th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>blast</th>\n",
       "      <th>ear</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>every</th>\n",
       "      <th>face</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>guest</th>\n",
       "      <th>little</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>obnoxious</th>\n",
       "      <th>really</th>\n",
       "      <th>recourse</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.37082</td>\n",
       "      <td>0.193509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506765</td>\n",
       "      <td>0.506765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.506765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.264451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.250651</td>\n",
       "      <td>0.317919</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.165903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.162819</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "X_train[6]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx ear worm go away'"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names()).loc[1].sort_values(ascending=False)\r\n",
    "# Burada virginamerica bütün cümlelerde geçmesine rağmen TF IDF onun ağırlığını düşürdü. "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bad              0.506765\n",
       "big              0.506765\n",
       "thing            0.506765\n",
       "really           0.399540\n",
       "virginamerica    0.264451\n",
       "aggressive       0.000000\n",
       "nearly           0.000000\n",
       "worm             0.000000\n",
       "vx               0.000000\n",
       "trip             0.000000\n",
       "today            0.000000\n",
       "time             0.000000\n",
       "take             0.000000\n",
       "recourse         0.000000\n",
       "obnoxious        0.000000\n",
       "need             0.000000\n",
       "mean             0.000000\n",
       "must             0.000000\n",
       "amp              0.000000\n",
       "little           0.000000\n",
       "guest            0.000000\n",
       "go               0.000000\n",
       "fly              0.000000\n",
       "face             0.000000\n",
       "every            0.000000\n",
       "entertainment    0.000000\n",
       "ear              0.000000\n",
       "blast            0.000000\n",
       "away             0.000000\n",
       "another          0.000000\n",
       "yes              0.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit (system)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "43a9cd95aa030499ef934e4da31e07dde54f3fc3a1fd0cc4aeef7f16459ee2ac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}