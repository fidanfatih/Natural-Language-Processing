{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## NLP Lab-1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word_embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#!pip install gensim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#pip install --upgrade pip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import numpy as np\r\n",
    "from gensim.models import Word2Vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "text = open(\"hurriyet.txt\", \"r\", encoding=\"utf8\")\r\n",
    "text = text.read()\r\n",
    "list_sent = text.split(\"\\n\")\r\n",
    "\r\n",
    "corpus = []\r\n",
    "\r\n",
    "for sent in list_sent:\r\n",
    "    corpus.append(sent.split())\r\n",
    "# Corpus taki cümleleri split ile ayırarak liste halinde corpus listesine ekliyoruz."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(corpus[:5])\r\n",
    "# İlk beş cümleye bakıyoruz."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['iran', 'devlet', 'televizyonu', 'ülkedeki', 'eyaletin', 'sinde', 'yapılan', 'reformcuları', 'protesto', 'amaçlı', 'yürüyüşlere', 'milyonlarca', 'kişinin', 'katıldığını', 'bildirdi'], ['gösterilerde', 'fitnecilere', 'ölüm', 'münafıklara', 'ölüm', 'abd', 'ye', 'ölüm', 'ingiltere', 'ye', 'ölüm', 'sloganları', 'atıldı'], ['dini', 'lider', 'ali', 'hamaney', 've', 'cumhurbaşkanı', 'mahmud', 'ahmedinejad', 'ı', 'destekleyen', 'iranlılar', 'son', 'olaylarda', 'yeğeni', 'öldürülen', 'mir', 'hüseyin', 'musevi', 'başta', 'olmak', 'üzere', 'muhalefet', 'liderlerini', 'kınadılar'], ['musevi', 'ye', 'ölüm', 've', 'idam', 'idam', 'sloganları', 'duyuldu'], ['muhalefet', 'liderleri', 'kaçtı', 'mı', 'aşure', 'günü', 'yaşanan', 'çatışmalarda', 'devlet', 'kaynaklarına', 'göre', 'u', 'terörist', 'olmak', 'üzere', 'kişi', 'ölmüştü']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model = Word2Vec(corpus, vector_size = 100, window =5, min_count=5, sg=1)\r\n",
    "# windows = 5; kelimeyi o kelimenin sağında ve solundaki beş kelime kalıbı ile öğren demek. Örnek: 'iran', 'devlet', 'televizyonu', 'ülkedeki', 'eyaletin','sinde' burada İran kelimesini ele alırsak sağındaki beş kelime ile kullanımını değerlendiriyor, bu değerlendirmeyi bütün corpus taki (Iran birçok cümlede geçiyor) kullanımlarına bakarak bu kelimere olasılık değeri atıyor. \r\n",
    "# vector_size = 100 ile 100 lük vectorler oluşturuyoruz.\r\n",
    "# min_count=5;  kelime corpus ta 5 den az kullanılmıs ise onu öğrenme demek.\r\n",
    "# sg=1; hedef kelimeyi çıkararak sağında ve solundaki birer kelime ile bulmaya çalışıyor. (ben .... geldim (hedef ... = eve)). Bu modele kelimenin farklı anlamlarını öğrenmesini sağlır\r\n",
    "# Word2Vec kelime kalıplarını (isim tamlaması, sıfat tamlaması, deyim vb.) öğrenirken yukarıdaki parametreleri kullanarak kelime ile aynı anlamda olan veya o kelimeden sonra veya önce gelen kelimelerin kullanılma olasılıklarına göre öğreniyor ve tahminlemeyi ona göre yapıyor.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "model.wv.most_similar(\"birşey\")\r\n",
    "# benzer veya birlikte kullanılan kelimeleri kontrol ediyoruz."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('muydu', 0.8389551639556885),\n",
       " ('sorunum', 0.837519645690918),\n",
       " ('şeyim', 0.8348615169525146),\n",
       " ('derler', 0.828641414642334),\n",
       " ('yapmadım', 0.8213338851928711),\n",
       " ('utanılacak', 0.8150485754013062),\n",
       " ('nedenimiz', 0.8140127658843994),\n",
       " ('stratejimiz', 0.8140026926994324),\n",
       " ('birilerine', 0.8134835362434387),\n",
       " ('olmadığımı', 0.8125412464141846)]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model.wv.most_similar(\"mavi\")\r\n",
    "# benzer veya birlikte kullanılan kelimeleri kontrol ediyoruz."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('marmara', 0.9025732278823853),\n",
       " ('gemisine', 0.6759141087532043),\n",
       " ('baskınıyla', 0.6467535495758057),\n",
       " ('baskınına', 0.6257569193840027),\n",
       " ('baskınının', 0.6176883578300476),\n",
       " ('dökme', 0.6128013730049133),\n",
       " ('filo', 0.5965344309806824),\n",
       " ('filosundaki', 0.5814570784568787),\n",
       " ('kurtlar', 0.5752832889556885),\n",
       " ('baskınından', 0.574649453163147)]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model.wv.most_similar(\"eve\")\r\n",
    "# benzer veya birlikte kullanılan kelimeleri kontrol ediyoruz."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('evine', 0.8153290748596191),\n",
       " ('dükkana', 0.7632506489753723),\n",
       " ('mağazaya', 0.7581343054771423),\n",
       " ('apartmana', 0.7415217161178589),\n",
       " ('arabaya', 0.7404642105102539),\n",
       " ('arabasına', 0.7240186929702759),\n",
       " ('hapishaneye', 0.7234700918197632),\n",
       " ('otobüse', 0.717900812625885),\n",
       " ('odasına', 0.7153779864311218),\n",
       " ('sinemaya', 0.7087869644165039)]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model.wv.most_similar(positive=['öğrenme', 'doktor'], negative=['tedavi'], topn=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('psikoloji', 0.6641547083854675)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model.wv.most_similar(positive=['ankara', 'almanya'], negative=['berlin'], topn=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('rusya', 0.6058714389801025)]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Glove"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "# glove2word2vec un Word2Vec ten farkı vektör olasılıklarını sürekli güncelliyor."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#glove_model = \"glove.6B.100d.txt\"\r\n",
    "#word2vec = \"glove.6B.100d.word2vec\"\r\n",
    "#glove2word2vec(glove_model, word2vec)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "model2 = KeyedVectors.load_word2vec_format(word2vec)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.word2vec'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-719e10bef93e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m         \"\"\"\n\u001b[1;32m-> 1630\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1631\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1892\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.word2vec'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.most_similar(\"blue\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('red', 0.8435065150260925),\n",
       " ('black', 0.8065882921218872),\n",
       " ('pink', 0.786806046962738),\n",
       " ('green', 0.7868044376373291),\n",
       " ('purple', 0.7847141623497009),\n",
       " ('yellow', 0.7777267694473267),\n",
       " ('gray', 0.7732225060462952),\n",
       " ('bright', 0.743113100528717),\n",
       " ('white', 0.7365070581436157),\n",
       " ('dark', 0.717019259929657)]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.most_similar(\"doctor\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('physician', 0.7673240303993225),\n",
       " ('nurse', 0.75215083360672),\n",
       " ('dr.', 0.7175194025039673),\n",
       " ('doctors', 0.7080884575843811),\n",
       " ('patient', 0.7074184417724609),\n",
       " ('medical', 0.6995992660522461),\n",
       " ('surgeon', 0.6905338168144226),\n",
       " ('hospital', 0.6900930404663086),\n",
       " ('psychiatrist', 0.658909797668457),\n",
       " ('dentist', 0.6447421312332153)]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.most_similar(positive=[\"woman\", \"father\"], negative=[\"man\"], topn=1)\r\n",
    "# \"father\" \"man\" ise \"woman\" \"mother\" olur."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('mother', 0.9024619460105896)]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.most_similar(positive=[\"woman\", \"brother\"], negative=[\"man\"], topn=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('daughter', 0.8917792439460754)]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.most_similar(positive=[\"ankara\", \"germany\"], negative=[\"berlin\"], topn=1)\r\n",
    "# germany den berlin i çıkrırsak, ankara yurkey verir."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('turkey', 0.81471186876297)]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.most_similar(positive=[\"teach\", \"doctor\"], negative=[\"treat\"], topn=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('teacher', 0.7610154151916504)]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Not. Word Embedding lerin amacı modele kelime kalıpları öğretmek. "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit (system)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "43a9cd95aa030499ef934e4da31e07dde54f3fc3a1fd0cc4aeef7f16459ee2ac"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}